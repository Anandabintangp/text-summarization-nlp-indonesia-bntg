#BERT model
#matrix ROUGE
#menggunakan data yg sudah dicleaning
#menggunakan subset data dari 20-30% populasi total data train

%pip install datasets rouge-score

df_validate.head()

df_validate['clean_article'] = df_validate['clean_article'].apply(lambda x: " ".join(sum(x,[])))
df_validate['clean_summary'] = df_validate['clean_summary'].apply(lambda x: " ".join(sum(x,[])))

df_validate['clean_article'].iloc[0]

df_test['clean_article'].iloc[0]

from datasets import Dataset

df = dftrain_sample[["clean_article", "clean_summary"]]
eval_df = df_validate[["clean_article", "clean_summary"]]
dataset = Dataset.from_pandas(df)
eval_dataset = Dataset.from_pandas(eval_df)

model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "indobenchmark/indobert-base-p1", "indobenchmark/indobert-base-p1"
)

model.config.decoder_start_token_id = tokenizer.cls_token_id
model.config.pad_token_id = tokenizer.pad_token_id

%pip install accelerate>=0.26.0

def preprocess_function(examples):
    inputs = tokenizer(examples['clean_article'], truncation=True, padding="max_length", max_length=512)
    targets = tokenizer(examples['clean_summary'], truncation=True, padding="max_length", max_length=128)
    inputs['labels'] = targets['input_ids']
    return inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)
eval_tokenized_dataset = eval_dataset.map(preprocess_function, batched=True)


training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=6,
    per_device_eval_batch_size=1,
    num_train_epochs=10,
    logging_steps=10,
    eval_strategy="epoch",
    logging_dir='/results/runs',
    save_strategy='best',
    load_best_model_at_end=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=eval_tokenized_dataset,
)

trainer.train()

trainer.evaluate()

trainer.save_model('./abstractive-summary')
