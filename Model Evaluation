df_test['clean_article'] = df_test['clean_article'].apply(lambda x: " ".join(sum(x,[])))
df_test['clean_summary'] = df_test['clean_summary'].apply(lambda x: " ".join(sum(x,[])))
model = EncoderDecoderModel.from_pretrained('./abstractive-summary')
tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')
from transformers import GenerationConfig
model.generation_config = GenerationConfig.from_model_config(model.config)

import random
from rouge_score import rouge_scorer

sum_rouge1 = 0
sum_rouge2 = 0
sum_rougeL = 0

avg_rouge1 = 0
avg_rouge2 = 0
avg_rougeL = 0

len_data_test = 3

for loop in range(len_data_test):

    i = random.randint(0, len(df_test['clean_article']))

    inputs = tokenizer(
    df_test['clean_article'].iloc[i],
    return_tensors="pt",
    truncation=True, padding="max_length", max_length=512
    )

    summary_ids = model.generate(
    **inputs,
    max_length=128,
    num_beams=4,
    early_stopping=True
    )

    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    ref_summary = df_test['clean_summary'].iloc[i]

    scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
    scores = scorer.score(ref_summary, summary)

    print(f"Data ID: {i}")
    print(f"Summary Reference:\n{ref_summary}")
    print(f"Model Summaries:\n{summary}\n")
    print(f"ROUGE-1 Score: {scores["rouge1"].fmeasure}")
    print(f"ROUGE-2 Score: {scores["rouge2"].fmeasure}")
    print(f"ROUGE-L Score: {scores["rougeL"].fmeasure}\n\n")

    sum_rouge1 += scores["rouge1"].fmeasure
    sum_rouge2 += scores["rouge2"].fmeasure
    sum_rougeL += scores["rougeL"].fmeasure

    if loop == len_data_test - 1:
        break

avg_rouge1 = sum_rouge1 / len_data_test
avg_rouge2 = sum_rouge2 / len_data_test
avg_rougeL = sum_rougeL / len_data_test

print(f"Average Rouge Score for {len_data_test} Data:")
print(f"ROUGE-1: {avg_rouge1}")
print(f"ROUGE-2: {avg_rouge2}")
print(f"ROUGE-L: {avg_rougeL}")
